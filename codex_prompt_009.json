{
  "sequence": "009",
  "id": "WS-CODEX-009",
  "phase": "AI Runtime Bridge",
  "title": "Replace AIProvider Mock with WebLLM-Powered Streaming Engine",
  "summary": "Wire the AIProvider to the real WebLLM engine, inject Safety Contract and Creativity Boundaries, and expose a clean station-aware streaming API.",
  "instructions": "You must respect the SAFETY_CONTRACT.md and docs/CREATIVITY_BOUNDARY_SPEC.md in every AI call.\n\nGoal for this task:\nRemove the AIProvider mock/stub and connect the UI to the real WebLLM runtime via a station-aware streaming API.\n\nPerform these steps:\n\n1) Locate AIProvider\n   - Find the AIProvider implementation (for example app/src/runtime/ai/AIProvider.tsx).\n   - If it does not exist yet, create it in a sensible location under app/src/runtime/ai/.\n\n2) Replace mock logic\n   - Remove any string-splitting or fake streaming behavior.\n   - Import generateStream(prompt: string) from webllm_loader.ts (implemented in WS-CODEX-010).\n\n3) Define AIProvider context shape\n   - The context should expose at least:\n     - streamStationCall(stationKey: string, payload: { systemPrompt: string; userPrompt: string }): AsyncGenerator<{ token: string; done: boolean }>\n   - Optionally also expose:\n     - lastError, lastStationKey, isStreaming flags.\n\n4) Inject Safety Contract and Creativity Boundaries\n   - Create a helper (for example app/src/runtime/ai/prompt_builder.ts) that:\n     - Loads or inlines the key rules from SAFETY_CONTRACT.md.\n     - Loads or inlines the key constraints from CREATIVITY_BOUNDARY_SPEC.md.\n     - Combines them with the station-specific systemPrompt and the userPrompt into a single finalPrompt string.\n   - Ensure the finalPrompt is clearly structured and commented for future maintenance.\n\n5) Station-aware calling\n   - streamStationCall should:\n     - Accept a stationKey like 'idea', 'build', 'test', 'memory', 'reflect', 'share', or 'replay'.\n     - Use prompt_builder to construct finalPrompt with station name and rules.\n     - Call generateStream(finalPrompt) and return its async generator.\n\n6) Token and time limits\n   - Add a simple layer that:\n     - Enforces a maximum token budget per call (documented in comments).\n     - Enforces a reasonable timeout for a request; if exceeded, surface a friendly error.\n\n7) React hook\n   - Implement a useAI() hook that returns the AIProvider context.\n   - Guard against usage outside the provider with a clear error message.\n\nDo not design each station systemPrompt here beyond basic placeholders; detailed station prompt content can be refined in later tasks. Focus on plumbing, safety enforcement, and streaming behavior.",
  "acceptance_criteria": [
    "AIProvider no longer uses any mock or stubbed LLM behavior.",
    "streamStationCall(stationKey, payload) exists and delegates to generateStream via a structured finalPrompt.",
    "Safety Contract rules and Creativity Boundary constraints are injected into every AI call via a prompt builder helper.",
    "A useAI() hook is available and returns the provider context, with proper error handling outside the provider.",
    "Token and timeout limits are implemented and documented, and streaming works for at least one Station in the UI."
  ]
}