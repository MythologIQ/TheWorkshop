# AI Behavior and Safety

This document explains how AI calls are wired today and how the Safety Contract + creativity boundaries keep responses kid-safe.

## AI request path

1. Components that need language help call `useAI()` (from `app/src/runtime/ai/AIProvider.tsx`) and call `stream(prompt)` or other helpers to receive token streams.
2. `AIProvider` relies on `webllm_loader.ts` to load the model defined in `app/src/runtime/llm/model_config.ts`, applying the currently persisted `modelId` and `performanceModeId` stored by `llmSettingsStore.ts`.
3. `webllm_loader.ts` wraps a simple `WebLLMEngine` stub that scrubs disallowed patterns (`/suicide/i`, `/violence/i`, etc.), enforces a `tokenCap` via `calculateTokenCap`, and tags uncertain text before yielding tokens.
4. `AIProvider` also prefixes every prompt with the Safety Contract reminder (`"Follow Workshop safety."`) so every station request reiterates the same guardrails.
5. `AIProvider` exposes `streamWithPersona`, which composes persona prompts from `app/src/runtime/ai/ai_personas.ts` (see `docs/AI_PERSONAS_SPEC.md`) so each station receives tone/detail/action cues layered on top of the standard safety reminders.

## Safety enforcement

- **SafetyContract** – The static contract (`docs/SAFETY_CONTRACT.md`) defines what kinds of content are off-limits. The contract is referenced in the AIProvider prompts, prompts generated by tutorials/templates, and in station copy whenever the model is summoned.
- **Creativity boundaries** – `projectStore`/`templates/applyTemplate.ts` clamp text lengths, ensure required fields exist, regenerate IDs (via `newId`), and sanitize station inputs before persisting. See `docs/CREATIVITY_BOUNDARY_SPEC.md` for the exact limits.
- **Token caps** – `llmSettingsStore.ts` calculates token caps and stores performance mode choices with localStorage persistence. `webllm_loader.ts` respects these caps and never streams more tokens than allowed.
- **Telemetry & privacy** – Telemetry increments counters (projects created, station visits, tutorials completed) in `telemetryStore.ts` and surfaces them in `DiagnosticsPage.tsx`. Nothing is sent off-device.

## Limitations & future work

- The current loader uses a stubbed `WebLLMEngine` and does not fetch remote models; future prompts will replace it with a real WebLLM integration but will keep the same safety/performance hooks.
- Telemetry remains a simple counter system; future work could add richer analytics, but any addition must stay local-only and transparent via the Diagnostics UI.
- Tutorials, templates, and stations must continue to coordinate with the SafetyContract when they construct prompts—any new prompt should restate the guardrails so they remain explicit.
