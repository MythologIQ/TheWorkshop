{
  "sequence": 11,
  "id": "WS-CODEX-011",
  "phase": "Phase 0 \u2013 Runtime foundation",
  "title": "Wire AIProvider to use WebLLM generateStream and station-aware calls",
  "summary": "Connect the React AIProvider to the new WebLLM runtime and provide a simple station-aware callStationModel API for the rest of the app.",
  "instructions": "You are continuing work on The Workshop runtime. The WebLLM engine loader (webllm_loader.ts) now exposes a generateStream(prompt) async generator.\n\nGoal for this task:\nWire the AIProvider (React context) to use generateStream and expose a small, station-aware API for making AI calls.\n\nPerform the following steps:\n\n1) Locate or create AIProvider\n   - Find the file that defines the AI context/provider (for example app/src/runtime/ai/AIProvider.tsx or similar).\n   - If it does not exist, create a minimal AIProvider in a sensible location under app/src/runtime/ai/.\n   - The provider should:\n     - Hold any needed AI-related state (loading flags, errors, last response).\n     - Expose a context value with AI call functions.\n\n2) Add station-aware call API\n   - Design and implement a function with a signature similar to:\n     - callStationModel(stationKey: string, payload: { systemPrompt: string; userPrompt: string }): AsyncGenerator<{ token: string; done: boolean }>\n   - stationKey is a string like \"idea\", \"build\", \"test\", etc.\n   - payload.systemPrompt and payload.userPrompt will be composed by higher-level station code.\n\n3) Compose prompts inside AIProvider\n   - Inside callStationModel:\n     - Build a full prompt string that includes:\n       - Station role and key.\n       - Safety reminders from the Safety Contract (docs/SAFETY_CONTRACT.md).\n       - Creativity boundary reminders (docs/CREATIVITY_BOUNDARY_SPEC.md).\n       - The station-specific system content from payload.systemPrompt.\n       - The user-specific content from payload.userPrompt.\n     - Keep the composition simple and clearly commented so future changes are easy.\n\n4) Delegate to generateStream\n   - Use generateStream(fullPrompt) from webllm_loader.ts to produce the output tokens.\n   - Expose this as an async generator so station components can stream results into their UIs.\n\n5) Export a React hook\n   - Add a useAI() hook that returns the context value, including callStationModel.\n   - Ensure null/undefined context is guarded with a clear error message if used outside the provider.\n\n6) Wrap the app with AIProvider\n   - Ensure the top-level App component (or router root) is wrapped with AIProvider so all stations can access useAI().\n\nDo not design the station prompts themselves here; that will be handled in separate tasks. Focus on wiring and context structure.",
  "acceptance_criteria": [
    "An AIProvider component exists and is used to wrap the app.",
    "A useAI() hook is available and returns a context value.",
    "callStationModel(stationKey, payload) is implemented and composes a full prompt including safety and boundary reminders.",
    "callStationModel delegates to generateStream from webllm_loader.ts and returns a streaming async generator.",
    "All new code is commented to explain how safety and creativity boundaries are injected."
  ]
}