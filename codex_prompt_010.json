{
  "sequence": 10,
  "id": "WS-CODEX-010",
  "phase": "Phase 0 \u2013 Runtime foundation",
  "title": "Replace stub LLM runtime with real WebLLM engine",
  "summary": "Swap out the stubbed LLM engine for a real WebLLM-based engine and expose a streaming API that the AIProvider can use.",
  "instructions": "You are enhancing The Workshop (kids-first AI creative studio). The current LLM runtime in app/src/runtime/llm/webllm_loader.ts is a stub that just splits the prompt and streams tokens back.\n\nGoal for this task:\nReplace the stub engine with a real WebLLM engine that can run a small local model in the browser and expose a streaming generate API.\n\nDo the following steps in order:\n\n1) Install WebLLM dependency\n   - Ensure @mlc-ai/web-llm is installed in the project.\n   - If package.json does not include it, add it and assume npm install will be run outside this task.\n\n2) Define a model catalog\n   - Create app/src/runtime/llm/model_manifest.ts (or update if it exists).\n   - Export a MODEL_CATALOG object listing at least:\n     - phi3_mini: default small model id (for example Phi-3-mini-4k-instruct-q4)\n     - qwen2_15b: backup model id (Qwen2.5-1.5B-instruct-q4 or similar)\n     - tiny_llama: tiny fallback model id (TinyLlama-1.1B-chat-q4 or similar)\n   - These ids should match plausible WebLLM model identifiers but the exact assets will be configured later.\n\n3) Implement WebLLM engine loader\n   - In app/src/runtime/llm/webllm_loader.ts:\n     - Remove or disable the existing createStubEngine implementation.\n     - Import CreateWebWorkerMLCEngine (or the appropriate WebLLM API) from @mlc-ai/web-llm.\n     - Implement a getEngine() function that lazily creates and caches a single WebLLM engine instance using a Web Worker.\n     - Configure the engine with the default model from MODEL_CATALOG (phi3_mini).\n\n4) Implement a streaming generate API\n   - Export an async generator function generateStream(prompt: string): AsyncGenerator<{ token: string; done: boolean }> from webllm_loader.ts.\n   - It should:\n     - Ensure the engine is initialized via getEngine().\n     - Send the user prompt using a simple chatCompletion or equivalent API.\n     - Yield chunks of text as { token, done: false } as they arrive.\n     - Yield a final { token: \"\", done: true } when the stream is complete.\n   - Add simple guards for maximum tokens and a reasonable timeout to support low-end devices.\n\n5) Add basic error handling\n   - If the engine fails to initialize or a generation error occurs:\n     - Log a concise error to the console.\n     - Throw a controlled error or yield a small apology message so the UI can handle it gracefully.\n\nDo not modify any station UI or project logic in this task. Focus only on the LLM runtime and its public streaming interface.",
  "acceptance_criteria": [
    "webllm_loader.ts no longer uses the string-splitting stub implementation.",
    "A MODEL_CATALOG exists listing at least phi3_mini, qwen2_15b, and tiny_llama identifiers.",
    "getEngine() initializes a single WebLLM engine lazily and caches it.",
    "generateStream(prompt) is implemented as an async generator yielding { token, done } chunks.",
    "Errors in engine initialization or generation are handled gracefully and documented with comments."
  ]
}